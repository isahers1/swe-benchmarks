{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SWE-Benchmark with LangSmith\n",
    "\n",
    "SWE-Benchmark is one of the most popular (and difficult!) benchmarks for developers to test their coding agents against. In this walkthrough we will show you how to load the SWE-benchmark dataset into LangSmith and easily run evals on it, allowing you to have much better visibility into your agents behaviour then using the off-the-shelf SWE-benchmark eval suite. This allows you to pin specific problems quicker and iterate on your agent rapidly to improve performance!\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "To load the data, we will pull the `dev` split from Hugging Face, but for your use case you may wish to pull one of the `test`, or `train` splits, and if you want to combine multiple splits you can use `pd.concat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'dev': 'data/dev-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet', 'train': 'data/train-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/princeton-nlp/SWE-bench/\" + splits[\"dev\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing the 'version' column\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "This is a very important step! If you skip, the rest of the code WILL NOT WORK! \n",
    "</div>\n",
    "\n",
    "The `version` column contains all string values but all are in float format so they get converted to floats when you upload the CSV to create a LangSmith dataset. Although you can convert the values to strings during your experiments, the issue arises with values like `\"0.10\"`. When getting converted to a float, you get the value `0.1`, which would become `\"0.1\"` if you converted it to a string - causing a key error during execution of your proposed patch.\n",
    "\n",
    "In order to fix this, we need LangSmith to stop trying to convert the `version` column to floats. In order to do this, we can just append a string prefix to each of them that is not float compatible. We then need to split on this prefix when doing evaluation to get the actual `version` value. The prefix we choose here is the string `\"version:\"`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The ability to select column types when uploading a CSV to LangSmith will be added in the future to avoid having to use this workaround.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['version'] = df['version'].apply(lambda x: f\"version:{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to LangSmith\n",
    "\n",
    "### Save to CSV\n",
    "\n",
    "To upload the data to LangSmith, we first need to save it to a CSV, which we can do using the `to_csv` function provided by pandas. Make sure to save this file somewhere that is easily accesible to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./../swe-benchmark.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload CSV to LangSmith\n",
    "\n",
    "We are now ready to upload the CSV to LangSmith. Once you are on the LangSmith website (smith.langchain.com), go to the `Datasets & Testing` tab on the left side navigation bar, and then click the `+ New Dataset` button in the top right corner.\n",
    "\n",
    "Then click the `Upload CSV` button on the top, and select the CSV file you saved in the previous step. You can then give your dataset a name and description.\n",
    "\n",
    "Next, select `Key-Value` as the dataset type. Lastly head to the `Create Schema` section and add ALL OF THE KEYS as `Input fields`. There are no `Output fields` in this example because our evaluator is not comparing against a reference, but instead will run the output of our experiments in docker containers to ensure that the code actually solves the PR issue.\n",
    "\n",
    "Once you have populated the `Input fields` (and left the `Output fields` empty!) you can click the blue `Create` button in the top right corner, and your dataset will be created!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset split for quicker testing\n",
    "\n",
    "Since running the SWE-benchmark evaluator takes a long time when run on all examples, you can create a \"test\" split for quickly testing the evaluator and your code. This video shows you how to do so (to get to the starting page of the video, just click on your dataset created above and go to the `Examples` tab):\n",
    "\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"creating-split.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our prediction function\n",
    "\n",
    "Running evaluation over SWE-benchmark works a little differently than most evals you will typically run on LangSmith since we don't have a reference output. Because of this, we first generate all of our outputs without running an evaluator (note how the `evaluate` call doesn't have the `evaluators` parameter set). In this case we returned a dummy predict function, but you can insert your agent logic inside the `predict` function to make it work as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'perfect-lip-22' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a9bffcdf-1dfe-4aef-8805-8806f0110067/compare?selectedSessions=182de5dc-fc9d-4065-a3e1-34527f952fd8\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 24.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "def predict(inputs: dict):\n",
    "    return {\"instance_id\":inputs['instance_id'],\"model_patch\":\"None\",\"model_name_or_path\":\"test-model\"}\n",
    "\n",
    "result = evaluate(\n",
    "    predict,\n",
    "    data=client.list_examples(dataset_id=\"a9bffcdf-1dfe-4aef-8805-8806f0110067\",splits=[\"test\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our predictions using SWE-benchmark\n",
    "\n",
    "Now we can run the following code to run the predicted patches we generated above in Docker. This code is edited slightly from the `SWE-bench` [run_evaluation.py](https://github.com/princeton-nlp/SWE-bench/blob/main/swebench/harness/run_evaluation.py) file.\n",
    "\n",
    "Basically, the code sets up docker images to run the predictions in parallel, which greatly reduces the time needed for evaluation. This screenshot explains the basics of how `SWE-bench` does evaluation under the hood. To understand it in full, make sure to read through the code in the [GitHub repository](https://github.com/princeton-nlp/SWE-bench).\n",
    "\n",
    "![Eval Diagram](evaluation.png)\n",
    "\n",
    "The function `convert_runs_to_langsmith_feedback` converts the logs generated by the docker file into a nice .json file that contains feedback in the typical key/score method of LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swebench.harness.run_evaluation import run_instances\n",
    "import resource\n",
    "import docker\n",
    "from swebench.harness.docker_utils import list_images, clean_images\n",
    "from swebench.harness.docker_build import build_env_images\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "RUN_EVALUATION_LOG_DIR = Path(\"logs/run_evaluation\")\n",
    "LANGSMITH_EVALUATION_DIR = './langsmith_feedback/feedback.json'\n",
    "\n",
    "def convert_runs_to_langsmith_feedback(\n",
    "        predictions: dict,\n",
    "        full_dataset: list,\n",
    "        run_id: str\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Convert logs from docker containers into LangSmith feedback.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): Predictions dict generated by the model\n",
    "        full_dataset (list): List of all instances\n",
    "        run_id (str): Run ID\n",
    "    \"\"\"\n",
    "    feedback_for_all_instances = {}\n",
    "\n",
    "    for instance in full_dataset:\n",
    "        feedback_for_instance = []\n",
    "        instance_id = instance['instance_id']\n",
    "        prediction = predictions[instance_id]\n",
    "        if prediction.get(\"model_patch\", None) in [\"\", None]:\n",
    "            # Prediction returned an empty patch\n",
    "            feedback_for_all_instances[prediction['run_id']] = [{\"key\":\"non-empty-patch\",\"score\":0},\n",
    "                                                                {\"key\":\"completed-patch\",\"score\":0},\n",
    "                                                                {\"key\":\"resolved-patch\",\"score\":0}]\n",
    "            continue\n",
    "        feedback_for_instance.append({\"key\":\"non-empty-patch\",\"score\":1})\n",
    "        report_file = (\n",
    "            RUN_EVALUATION_LOG_DIR\n",
    "            / run_id\n",
    "            / prediction[\"model_name_or_path\"].replace(\"/\", \"__\")\n",
    "            / prediction['instance_id']\n",
    "            / \"report.json\"\n",
    "        )\n",
    "        if report_file.exists():\n",
    "            # If report file exists, then the instance has been run\n",
    "            feedback_for_instance.append({\"key\":\"completed-patch\",\"score\":1})\n",
    "            report = json.loads(report_file.read_text())\n",
    "            # Check if instance actually resolved the PR\n",
    "            if report[instance_id][\"resolved\"]:\n",
    "                feedback_for_instance.append({\"key\":\"resolved-patch\",\"score\":1})\n",
    "            else:\n",
    "                feedback_for_instance.append({\"key\":\"resolved-patch\",\"score\":0})\n",
    "        else:\n",
    "            # The instance did not run succesfully\n",
    "            feedback_for_instance += [{\"key\":\"completed-patch\",\"score\":0},{\"key\":\"resolved-patch\",\"score\":0}]\n",
    "        feedback_for_all_instances[prediction['run_id']] = feedback_for_instance\n",
    "\n",
    "    os.makedirs(os.path.dirname(LANGSMITH_EVALUATION_DIR), exist_ok=True)\n",
    "    with open(LANGSMITH_EVALUATION_DIR, 'w') as json_file:\n",
    "        json.dump(feedback_for_all_instances, json_file)\n",
    "\n",
    "def evaluate_predictions(\n",
    "        dataset: list,\n",
    "        predictions: list,\n",
    "        max_workers: int,\n",
    "        force_rebuild: bool,\n",
    "        cache_level: str,\n",
    "        clean: bool,\n",
    "        open_file_limit: int,\n",
    "        run_id: str,\n",
    "        timeout: int,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Run evaluation harness for the given dataset and predictions.\n",
    "    \"\"\"\n",
    "    # set open file limit\n",
    "    assert len(run_id) > 0, \"Run ID must be provided\"\n",
    "    resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))\n",
    "    client = docker.from_env()\n",
    "\n",
    "    existing_images = list_images(client)\n",
    "    print(f\"Running {len(dataset)} unevaluated instances...\")\n",
    "    # build environment images + run instances\n",
    "    build_env_images(client, dataset, force_rebuild, max_workers)\n",
    "    run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout)\n",
    "\n",
    "    # clean images + make final report\n",
    "    clean_images(client, existing_images, cache_level, clean)\n",
    "\n",
    "    convert_runs_to_langsmith_feedback(predictions,dataset,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "predictions = {}\n",
    "for res in result:\n",
    "    predictions[res['run'].outputs['instance_id']] = {**res['run'].outputs,**{\"run_id\":str(res['run'].id)}}\n",
    "    dataset.append(res['run'].inputs['inputs'])\n",
    "for d in dataset:\n",
    "    d['version'] = d['version'].split(\":\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3 unevaluated instances...\n",
      "Base image sweb.base.arm64:latest already exists, skipping build.\n",
      "Base images built successfully.\n",
      "Total environment images to build: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building environment images: 100%|██████████| 2/2 [00:47<00:00, 23.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All environment images built successfully.\n",
      "Running 3 instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation error for sqlfluff__sqlfluff-884: >>>>> Patch Apply Failed:\n",
      "patch unexpectedly ends in middle of line\n",
      "patch: **** Only garbage was found in the patch input.\n",
      "\n",
      "Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-884/run_instance.log) for more information.\n",
      "Evaluation error for sqlfluff__sqlfluff-4151: >>>>> Patch Apply Failed:\n",
      "patch unexpectedly ends in middle of line\n",
      "patch: **** Only garbage was found in the patch input.\n",
      "\n",
      "Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-4151/run_instance.log) for more information.\n",
      "Evaluation error for sqlfluff__sqlfluff-2849: >>>>> Patch Apply Failed:\n",
      "patch: **** Only garbage was found in the patch input.\n",
      "patch unexpectedly ends in middle of line\n",
      "\n",
      "Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-2849/run_instance.log) for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:30<00:00, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All instances run.\n",
      "Cleaning cached images...\n",
      "Removed 0 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(dataset,predictions,max_workers=8,force_rebuild=False,cache_level=\"env\",clean=False \\\n",
    "                     ,open_file_limit=4096,run_id=\"test\",timeout=1_800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Evaluation to LangSmith\n",
    "\n",
    "Now, we can actually send our evaluation feedback to LangSmith by using the `evaluate_existing` function. Our evaluate function is incredibly simple in this case, because the `convert_runs_to_langsmith_feedback` function above made our life very easy by saving all the feedback to a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'perfect-lip-22' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a9bffcdf-1dfe-4aef-8805-8806f0110067/compare?selectedSessions=182de5dc-fc9d-4065-a3e1-34527f952fd8\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  1.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults perfect-lip-22>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate_existing\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def swe_bench_evaluator(run: Run, example: Example):\n",
    "    with open(LANGSMITH_EVALUATION_DIR, 'r') as json_file:\n",
    "        langsmith_eval = json.load(json_file)\n",
    "    return {\"results\": langsmith_eval[str(run.id)]}\n",
    "\n",
    "experiment_name = result.experiment_name\n",
    "evaluate_existing(experiment_name, evaluators=[swe_bench_evaluator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running, we can go to the experiments tab of our dataset, and check that our feedback keys were properly assigned. If they were, you should see something that resembles the following image:\n",
    "\n",
    "![LangSmith feedback](langsmith_feedback.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
